{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 준비\n",
    "\n",
    "base_model_path = 'skt/kogpt2-base-v2'\n",
    "\n",
    "base_model = AutoModel.from_pretrained(base_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_path,\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"pad_token\": DEFAULT_PAD_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    }\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './output_2_RM'\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'Ja-ck/Orca-DPO-Pairs-KO'\n",
    "data_path = 'AIdenU/orca_dpo_data_ko'\n",
    "dataset = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_dict = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "\n",
    "    data = {}\n",
    "    data['system'] = tmp['system']\n",
    "    data['prompt'] = tmp['question']\n",
    "    data['chosen'] = tmp['chosen']\n",
    "    data['rejected'] = tmp['rejected']\n",
    "\n",
    "    total_data_ranking2chosen.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_ranking2chosen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": \"\"\"\n",
    "<start_of_turn>user\n",
    "{system}\n",
    "\n",
    "### Input:\n",
    "{user_input}\n",
    "\n",
    "<start_of_turn>model\n",
    "{model_answer}\n",
    "\"\"\".lstrip(),\n",
    "    \"prompt_no_input\": \"\"\"\n",
    "<start_of_turn>user\n",
    "### Input:\n",
    "{user_input}\n",
    "\n",
    "<start_of_turn>model\n",
    "{model_answer}\n",
    "\"\"\".lstrip(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = total_data_ranking2chosen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_col_dict = {\n",
    "    'system':'system',\n",
    "    'user_input':'prompt',\n",
    "    'chosen':'chosen',\n",
    "    'rejected':'rejected',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RewardDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for reward model\n",
    "\n",
    "    Args:\n",
    "        dataset: dataset for reward model\n",
    "        tokenizer: tokenizer for reward model\n",
    "        max_length: max length of input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, data_col_dict, tokenizer: Callable, max_length: int) -> None:\n",
    "        super().__init__()\n",
    "        self.chosen = []\n",
    "        self.reject = []\n",
    "        \n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기\n",
    "                \n",
    "        for data in tqdm(dataset):\n",
    "            \n",
    "            if data.get(data_col_dict['system'], \"\") != \"\":\n",
    "                chosen = prompt_input.format_map({\n",
    "                    'system':data[data_col_dict['system']],\n",
    "                    'user_input':data[data_col_dict['user_input']],\n",
    "                    'model_answer':data[data_col_dict['chosen']],\n",
    "                }) + tokenizer.eos_token\n",
    "            else:\n",
    "                chosen = prompt_no_input.format_map({\n",
    "                    'user_input':data[data_col_dict['user_input']],\n",
    "                    'model_answer':data[data_col_dict['chosen']],\n",
    "                }) + tokenizer.eos_token\n",
    "            \n",
    "            chosen_token = tokenizer(chosen,\n",
    "                                     max_length=max_length,\n",
    "                                     padding=\"longest\",\n",
    "                                     truncation=True,\n",
    "                                     return_tensors=\"pt\")\n",
    "            self.chosen.append({\n",
    "                \"input_ids\": chosen_token['input_ids'][0],\n",
    "                \"attention_mask\": chosen_token['attention_mask'][0]\n",
    "            })\n",
    "\n",
    "            # reject = prompt + data['rejected'] + \"<|endoftext|>\"\n",
    "            \n",
    "            if data.get(data_col_dict['system'], \"\") != \"\":\n",
    "                reject = prompt_input.format_map({\n",
    "                    'system':data[data_col_dict['system']],\n",
    "                    'user_input':data[data_col_dict['user_input']],\n",
    "                    'model_answer':data[data_col_dict['rejected']],\n",
    "                }) + tokenizer.eos_token\n",
    "            else:\n",
    "                reject = prompt_no_input.format_map({\n",
    "                    'user_input':data[data_col_dict['user_input']],\n",
    "                    'model_answer':data[data_col_dict['rejected']],\n",
    "                }) + tokenizer.eos_token\n",
    "            \n",
    "            \n",
    "            reject_token = tokenizer(reject,\n",
    "                                     max_length=max_length,\n",
    "                                     padding=\"longest\",\n",
    "                                     truncation=True,\n",
    "                                     return_tensors=\"pt\")\n",
    "            self.reject.append({\n",
    "                \"input_ids\": reject_token['input_ids'][0],\n",
    "                \"attention_mask\": reject_token['attention_mask'][0]\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        length = len(self.chosen)\n",
    "        return length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.chosen[idx][\"input_ids\"], self.reject[idx][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for data and dataset\n",
    "import random\n",
    "random.seed(230319)\n",
    "# list_tmp = list(range(10))\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])\n",
    "\n",
    "# train_data = total_data_ranking2chosen[:-1000]  # 29000 학습\n",
    "# eval_data = total_data_ranking2chosen[-1000:0]  # 1000개만 평가\n",
    "\n",
    "train_data = total_data_ranking2chosen[:100]  # 29000 학습\n",
    "eval_data = total_data_ranking2chosen[100:130]  # 1000개만 평가\n",
    "\n",
    "\n",
    "train_dataset = RewardDataset(train_data, data_col_dict, tokenizer, max_len)\n",
    "eval_dataset = RewardDataset(eval_data, data_col_dict, tokenizer, max_len)\n",
    "\n",
    "# check\n",
    "idx = 10\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]['input_ids_j'][0]\n",
    "\n",
    "print(tokenizer.decode(train_dataset[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Reward model base class.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Reward model.\n",
    "        value_head (nn.Module): Value head to get reward score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_model: nn.Module,\n",
    "                 value_head: Optional[nn.Module] = None,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        if value_head is not None:\n",
    "            if value_head.out_features != 1:\n",
    "                raise ValueError(\"The value head of reward model's output dim should be 1!\")\n",
    "            self.value_head = value_head\n",
    "        else:\n",
    "            self.value_head = nn.Linear(base_model.config.n_embd, 1)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_states = outputs['last_hidden_state']\n",
    "        values = self.value_head(last_hidden_states)[:, :-1]\n",
    "        value = values.mean(dim=1).squeeze(1)    # ensure shape is (B)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RewardModel(base_model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForRewardDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids_chosen, input_ids_reject = zip(*instances)\n",
    "\n",
    "        input_ids_chosen = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids_chosen, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        input_ids_reject = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids_reject, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids_j\": input_ids_chosen,\n",
    "            \"attention_mask_j\": input_ids_chosen.ne(self.tokenizer.pad_token_id),\n",
    "            \"input_ids_k\": input_ids_reject,\n",
    "            \"attention_mask_k\": input_ids_reject.ne(self.tokenizer.pad_token_id),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForRewardDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        rewards_j = model(input_ids=inputs[\"input_ids_j\"],  attention_mask=inputs[\"attention_mask_j\"])[0]\n",
    "        rewards_k = model(input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\n",
    "        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n",
    "        if return_outputs:\n",
    "            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=save_dir, #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=1, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    eval_steps = 3, # Number of update steps between two evaluations.\n",
    "    save_steps=500, # after # steps model is saved\n",
    "    warmup_steps=5,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_state()\n",
    "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
