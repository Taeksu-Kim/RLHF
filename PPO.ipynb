{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = 'skt/kogpt2-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_path,\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"pad_token\": DEFAULT_PAD_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    }\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "reward_model = pipeline(\"text-classification\", model=base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_path = 'AIdenU/orca_dpo_data_ko'\n",
    "\n",
    "dataset = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": \"\"\"\n",
    "<start_of_turn>user\n",
    "{system}\n",
    "\n",
    "### Input:\n",
    "{user_input}\n",
    "\n",
    "<start_of_turn>model\n",
    "\"\"\".lstrip(),\n",
    "    \"prompt_no_input\": \"\"\"\n",
    "<start_of_turn>user\n",
    "### Input:\n",
    "{user_input}\n",
    "\n",
    "<start_of_turn>model\n",
    "\"\"\".lstrip(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class PPO_dataset(Dataset):\n",
    "    '''SFT dataset by wygo'''\n",
    "    def __init__(self, list_data_dict: list, system: str, user_input: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(PPO_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        ## format\n",
    "        system = 'system'  \n",
    "        user_input = 'question' \n",
    "\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기\n",
    "\n",
    "        # 입력\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            if example.get(user_input, \"\") != \"\":\n",
    "                tmp = prompt_input.format_map({\n",
    "                    'system':example[system],\n",
    "                    'user_input':example[user_input],\n",
    "                })\n",
    "            else:\n",
    "                tmp = prompt_no_input.format_map({\n",
    "                    'user_input':example[user_input],\n",
    "                })\n",
    "            sources.append(tmp)\n",
    "\n",
    "        # source data tokenized\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source만\n",
    "\n",
    "\n",
    "        ## 입력은 source, 출력은 source+target 이지만 학습은 target 부분만\n",
    "        input_ids = sources_tokenized[\"input_ids\"]\n",
    "        query = sources_tokenized[\"query\"]\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids,\n",
    "                         query=query)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.query = data_dict[\"query\"]\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        \"\"\"Tokenize a list of strings.\"\"\"\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                # max_length=tokenizer.model_max_length,\n",
    "                max_length=256,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "\n",
    "        return dict(\n",
    "            query=strings,\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i],\n",
    "                    query=self.query[i],\n",
    "                    )\n",
    "\n",
    "def collator(data):\n",
    "    return {key: [d[key] for d in data] for key in data[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_dict = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PPO_dataset(list_data_dict=list_data_dict, \n",
    "                            system='system',\n",
    "                            user_input='question',\n",
    "                            tokenizer=tokenizer)\n",
    "eval_dataset  = None  # eval은 안함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig\n",
    "from bitsandbytes.optim import AdamW\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "config = PPOConfig(\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=batch_size,\n",
    "    mini_batch_size=batch_size,\n",
    "    steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './output_3_PPO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=train_dataset,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    # \"max_length\": 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 4\n",
    "# output_max_length = 400\n",
    "output_max_length = 99\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ppo_trainer.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute reward score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = reward_model(texts)\n",
    "    # rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "    rewards = [torch.tensor(output[\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
